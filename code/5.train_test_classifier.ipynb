{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Final Models\n",
    "Using the \"best\" hyperparameters found in tuning, train the final models for both Count and TF-IDF vectorizers on the whole training dataset. Then evaluate the models on the \"unseen\" test dataset.\n",
    "\n",
    "Save these trained models for future use.\n",
    "\n",
    "*Note: See scripts in \"5.train_test_classifier\" folder for scripting implementation. The final training was run with a python script on the CHPC clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and stopwords\n",
    "train_data = pd.read_pickle('../data/train_data.pkl')\n",
    "test_data = pd.read_pickle('../data/test_data.pkl')\n",
    "\n",
    "\n",
    "with open('../data/stopwords.pkl', 'rb') as f:\n",
    "    stopwords = pickle.load(f)\n",
    "    \n",
    "# smaller subset for testing/demo\n",
    "train_data = train_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestSVM(object):\n",
    "    def __init__(self, train_data, test_data, ngram_range, max_df, min_df, vector, C, \n",
    "                 kernel, stopwords, title):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.vector = vector\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.stopwords = stopwords\n",
    "        self.title = title\n",
    "        self.scores = pd.DataFrame()\n",
    "    \n",
    "    def train_test_save(self):\n",
    "        X_train = self.train_data['text'].values\n",
    "        y_train = self.train_data['label'].values\n",
    "        X_test = self.test_data['text'].values\n",
    "        y_test = self.test_data['label'].values\n",
    "            \n",
    "        if self.vector == 'count':\n",
    "            vectorizer = CountVectorizer(ngram_range=self.ngram_range, \n",
    "                                         max_df=self.max_df, \n",
    "                                         min_df=self.min_df, \n",
    "                                         stop_words=self.stopwords)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=self.ngram_range, \n",
    "                                         max_df=self.max_df, \n",
    "                                         min_df=self.min_df, \n",
    "                                         stop_words=self.stopwords)\n",
    "            \n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        \n",
    "        dump(X_train_vec, '../data/%s_train_vec.joblib' %self.title.replace(' ', ''))\n",
    "        dump(X_test_vec, '../data/%s_test_vec.joblib' %self.title.replace(' ', ''))\n",
    "\n",
    "        clf = SVC(C=self.C, kernel=self.kernel, probability=True, gamma='scale')\n",
    "        clf.fit(X_train_vec, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train_vec)\n",
    "        y_train_prob = clf.predict_proba(X_train_vec)\n",
    "        y_train_prob = y_train_prob[:, 1]\n",
    "        train_scores = self.evaluate_results(y_train, y_train_pred, y_train_prob)\n",
    "\n",
    "        y_test_pred = clf.predict(X_test_vec)\n",
    "        y_test_prob = clf.predict_proba(X_test_vec)\n",
    "        y_test_prob = y_test_prob[:, 1]\n",
    "        test_scores = self.evaluate_results(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "        self.scores = self.create_scores_dataframe(train_scores, test_scores)\n",
    "        self.plot_roc_curve(y_test, y_test_prob)\n",
    "        self.save_scores_csv()\n",
    "        dump(clf, '../results/models/%s_model.joblib' % self.title.replace(' ',''))\n",
    "        return None\n",
    "    \n",
    "    def evaluate_results(self, y_true, y_pred, y_prob):\n",
    "        scores = {}\n",
    "        scores['ngram_range'] = [self.ngram_range]\n",
    "        scores['max_df'] = [self.max_df]\n",
    "        scores['min_df'] = [self.min_df]\n",
    "        scores['vector']= [self.vector]\n",
    "        scores['C'] = [self.C]\n",
    "        scores['Acc'] = [accuracy_score(y_true, y_pred)]\n",
    "        scores['recall'] = [recall_score(y_true, y_pred)]\n",
    "        scores['PPV'] = [precision_score(y_true, y_pred)]\n",
    "        scores['AUC'] = [roc_auc_score(y_true, y_prob)]\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def create_scores_dataframe(self, train_dict, test_dict):\n",
    "        train_df = pd.DataFrame(train_dict)\n",
    "        train_df['dataset'] = 'train'\n",
    "\n",
    "        test_df = pd.DataFrame(test_dict)\n",
    "        test_df['dataset'] = 'test'\n",
    "        eval_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "        return eval_df\n",
    "    \n",
    "    def save_scores_csv(self):\n",
    "        self.scores.to_csv('../results/final/%s_scores.csv' %self.title.replace(' ',''))\n",
    "        return None\n",
    "    \n",
    "    def plot_roc_curve(self, y_true, y_prob):\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        roc_name = '../results/images/%s_ROC_curve.png' %self.title.replace(' ','')\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='#D3D3D3')\n",
    "        plt.plot(fpr, tpr, color='#8B0000')\n",
    "        plt.suptitle('%s Receiver Operating Characteristic Curve' %self.title, fontsize=20, y=0.96)\n",
    "        plt.title('AUC = {:.4f}'.format(auc), fontsize=16)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.savefig(roc_name, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_count_params = {\n",
    "    'ngram_range': (1,2),\n",
    "    'max_df': 0.5,\n",
    "    'min_df': 0,\n",
    "    'C': 0.01,\n",
    "    'kernel': 'linear'\n",
    "}\n",
    "\n",
    "train_test = TrainTestSVM(train_data, test_data, ngram_range=svm_count_params['ngram_range'], \n",
    "                          max_df=svm_count_params['max_df'], min_df=svm_count_params['min_df'], \n",
    "                          vector='count', C=svm_count_params['C'], kernel=svm_count_params['kernel'], \n",
    "                          stopwords=stopwords, title='Count - SVM-demo')\n",
    "train_test.train_test_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tfidf_params = {\n",
    "    'ngram_range': (1,2),\n",
    "    'max_df': 0.25,\n",
    "    'min_df': 0,\n",
    "    'C': 1.2575,\n",
    "    'kernel': 'linear'\n",
    "}\n",
    "\n",
    "train_test = TrainTestSVM(train_data, test_data, ngram_range=svm_tfidf_params['ngram_range'], \n",
    "                          max_df=svm_tfidf_params['max_df'], min_df=svm_tfidf_params['min_df'], \n",
    "                          vector='tfidf', C=svm_tfidf_params['C'], kernel=svm_tfidf_params['kernel'], \n",
    "                          stopwords=stopwords, title='TF-IDF - SVM-demo')\n",
    "train_test.train_test_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestRF(object):\n",
    "    def __init__(self, train_data, test_data, ngram_range, max_df, min_df, vector, n_estimators, \n",
    "                 criterion, max_depth, stopwords, title):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.vector = vector\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.stopwords = stopwords\n",
    "        self.title = title\n",
    "        self.scores = pd.DataFrame()\n",
    "    \n",
    "    def train_test_save(self):\n",
    "        X_train = self.train_data['text'].values\n",
    "        y_train = self.train_data['label'].values\n",
    "        X_test = self.test_data['text'].values\n",
    "        y_test = self.test_data['label'].values\n",
    "            \n",
    "        if self.vector == 'count':\n",
    "            vectorizer = CountVectorizer(ngram_range=self.ngram_range, \n",
    "                                         max_df=self.max_df, \n",
    "                                         min_df=self.min_df, \n",
    "                                         stop_words=self.stopwords)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=self.ngram_range, \n",
    "                                         max_df=self.max_df, \n",
    "                                         min_df=self.min_df, \n",
    "                                         stop_words=self.stopwords)\n",
    "            \n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        \n",
    "        dump(X_train_vec, '../data/%s_train_vec.joblib' %self.title.replace(' ', ''))\n",
    "        dump(X_test_vec, '../data/%s_test_vec.joblib' %self.title.replace(' ', ''))\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=self.n_estimators, criterion=self.criterion, \n",
    "                                     max_depth=self.max_depth)\n",
    "        clf.fit(X_train_vec, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train_vec)\n",
    "        y_train_prob = clf.predict_proba(X_train_vec)\n",
    "        y_train_prob = y_train_prob[:, 1]\n",
    "        train_scores = self.evaluate_results(y_train, y_train_pred, y_train_prob)\n",
    "\n",
    "        y_test_pred = clf.predict(X_test_vec)\n",
    "        y_test_prob = clf.predict_proba(X_test_vec)\n",
    "        y_test_prob = y_test_prob[:, 1]\n",
    "        test_scores = self.evaluate_results(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "        self.scores = self.create_scores_dataframe(train_scores, test_scores)\n",
    "        self.plot_roc_curve(y_test, y_test_prob)\n",
    "        self.save_scores_csv()\n",
    "        dump(clf, '../results/models/%s_model.joblib' % self.title.replace(' ',''))\n",
    "        return None\n",
    "    \n",
    "    def evaluate_results(self, y_true, y_pred, y_prob):\n",
    "        scores = {}\n",
    "        scores['ngram_range'] = [self.ngram_range]\n",
    "        scores['max_df'] = [self.max_df]\n",
    "        scores['min_df'] = [self.min_df]\n",
    "        scores['vector']= [self.vector]\n",
    "        scores['n_estimators'] = [self.n_estimators]\n",
    "        scores['criterion'] = [self.criterion]\n",
    "        scores['max_depth'] = [self.max_depth]\n",
    "        scores['Acc'] = [accuracy_score(y_true, y_pred)]\n",
    "        scores['recall'] = [recall_score(y_true, y_pred)]\n",
    "        scores['PPV'] = [precision_score(y_true, y_pred)]\n",
    "        scores['AUC'] = [roc_auc_score(y_true, y_prob)]\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def create_scores_dataframe(self, train_dict, test_dict):\n",
    "        train_df = pd.DataFrame(train_dict)\n",
    "        train_df['dataset'] = 'train'\n",
    "\n",
    "        test_df = pd.DataFrame(test_dict)\n",
    "        test_df['dataset'] = 'test'\n",
    "        eval_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "        return eval_df\n",
    "    \n",
    "    def save_scores_csv(self):\n",
    "        self.scores.to_csv('../results/final/%s_scores.csv' %self.title.replace(' ',''))\n",
    "        return None\n",
    "    \n",
    "    def plot_roc_curve(self, y_true, y_prob):\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        roc_name = '../results/images/%s_ROC_curve.png' %self.title.replace(' ','')\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='#D3D3D3')\n",
    "        plt.plot(fpr, tpr, color='#8B0000')\n",
    "        plt.suptitle('%s Receiver Operating Characteristic Curve' %self.title, fontsize=20, y=0.96)\n",
    "        plt.title('AUC = {:.4f}'.format(auc), fontsize=16)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.savefig(roc_name, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_count_params = {\n",
    "    'ngram_range': (1,1),\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 0,\n",
    "    'n_estimators': 300,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 100\n",
    "}\n",
    "\n",
    "train_test = TrainTestRF(train_data, test_data, ngram_range=rf_count_params['ngram_range'], \n",
    "                         max_df=rf_count_params['max_df'], min_df=rf_count_params['min_df'], vector='count', \n",
    "                         n_estimators=rf_count_params['n_estimators'], criterion=rf_count_params['criterion'], \n",
    "                         max_depth=rf_count_params['max_depth'], stopwords=stopwords, title='Count - RF-demo')\n",
    "train_test.train_test_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tfidf_params = {\n",
    "    'ngram_range': (1,1),\n",
    "    'max_df': 0.5,\n",
    "    'min_df': 0,\n",
    "    'n_estimators': 300,\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': 100\n",
    "}\n",
    "\n",
    "train_test = TrainTestRF(train_data, test_data, ngram_range=rf_tfidf_params['ngram_range'], \n",
    "                         max_df=rf_tfidf_params['max_df'], min_df=rf_tfidf_params['min_df'], vector='tfidf', \n",
    "                         n_estimators=rf_tfidf_params['n_estimators'], criterion=rf_tfidf_params['criterion'], \n",
    "                         max_depth=rf_tfidf_params['max_depth'], stopwords=stopwords, title='TF-IDF - RF-demo')\n",
    "train_test.train_test_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestNN(object):\n",
    "    def __init__(self, train_data, test_data, ngram_range, max_df, min_df, vector, h1_nodes, optimizer, \n",
    "                 stopwords, title):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.vector = vector\n",
    "        self.h1_nodes = h1_nodes\n",
    "        self.optimizer = optimizer\n",
    "        self.stopwords = stopwords\n",
    "        self.title = title\n",
    "        self.scores = pd.DataFrame()\n",
    "    \n",
    "    def train_test_save(self):\n",
    "        X_train = self.train_data['text'].values\n",
    "        y_train = self.train_data['label'].values\n",
    "        X_test = self.test_data['text'].values\n",
    "        y_test = self.test_data['label'].values\n",
    "            \n",
    "        if self.vector == 'count':\n",
    "            vectorizer = CountVectorizer(ngram_range=self.ngram_range, \n",
    "                                         max_df=self.max_df, \n",
    "                                         min_df=self.min_df, \n",
    "                                         stop_words=self.stopwords)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=self.ngram_range, \n",
    "                                         max_df=self.max_df, \n",
    "                                         min_df=self.min_df, \n",
    "                                         stop_words=self.stopwords)\n",
    "            \n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        \n",
    "        dump(X_train_vec, '../data/%s_train_vec.joblib' %self.title.replace(' ', ''))\n",
    "        dump(X_test_vec, '../data/%s_test_vec.joblib' %self.title.replace(' ', ''))\n",
    "\n",
    "        n_dim = X_train_vec.shape[1]\n",
    "        early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3)\n",
    "                \n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.h1_nodes, activation='relu', input_dim=n_dim))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy']) \n",
    "        history = model.fit(X_train_vec, y_train, epochs=3000, validation_split=0.2, batch_size=100, \n",
    "                            callbacks=[early_stopping_monitor], verbose=0)\n",
    "\n",
    "        y_train_prob = model.predict(X_train_vec).flatten()\n",
    "        y_train_pred = model.predict_classes(X_train_vec).flatten()\n",
    "        train_scores = self.evaluate_results(y_train, y_train_pred, y_train_prob)\n",
    "\n",
    "        y_test_prob = model.predict(X_test_vec).flatten()\n",
    "        y_test_pred = model.predict_classes(X_test_vec).flatten()\n",
    "        test_scores = self.evaluate_results(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "        self.scores = self.create_scores_dataframe(train_scores, test_scores)\n",
    "        self.plot_roc_curve(y_test, y_test_prob)\n",
    "        self.save_scores_csv()\n",
    "        model.save('../results/models/%s_model.h5' % self.title.replace(' ',''))\n",
    "        return None\n",
    "    \n",
    "    def evaluate_results(self, y_true, y_pred, y_prob):\n",
    "        scores = {}\n",
    "        scores['ngram_range'] = [self.ngram_range]\n",
    "        scores['max_df'] = [self.max_df]\n",
    "        scores['min_df'] = [self.min_df]\n",
    "        scores['vector']= [self.vector]\n",
    "        scores['h1_nodes'] = [self.h1_nodes]\n",
    "        scores['optimizer'] = [self.optimizer]\n",
    "        scores['Acc'] = [accuracy_score(y_true, y_pred)]\n",
    "        scores['recall'] = [recall_score(y_true, y_pred)]\n",
    "        scores['PPV'] = [precision_score(y_true, y_pred)]\n",
    "        scores['AUC'] = [roc_auc_score(y_true, y_prob)]\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def create_scores_dataframe(self, train_dict, test_dict):\n",
    "        train_df = pd.DataFrame(train_dict)\n",
    "        train_df['dataset'] = 'train'\n",
    "\n",
    "        test_df = pd.DataFrame(test_dict)\n",
    "        test_df['dataset'] = 'test'\n",
    "        eval_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "        return eval_df\n",
    "    \n",
    "    def save_scores_csv(self):\n",
    "        self.scores.to_csv('../results/final/%s_scores.csv' %self.title.replace(' ',''))\n",
    "        return None\n",
    "    \n",
    "    def plot_roc_curve(self, y_true, y_prob):\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        roc_name = '../results/images/%s_ROC_curve.png' %self.title.replace(' ','')\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='#D3D3D3')\n",
    "        plt.plot(fpr, tpr, color='#8B0000')\n",
    "        plt.suptitle('%s Receiver Operating Characteristic Curve' %self.title, fontsize=20, y=0.96)\n",
    "        plt.title('AUC = {:.4f}'.format(auc), fontsize=16)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.savefig(roc_name, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_count_params = {\n",
    "    'ngram_range': (1,2),\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 0,\n",
    "    'h1_nodes': 128,\n",
    "    'optimizer': 'Adadelta'\n",
    "}\n",
    "\n",
    "train_test = TrainTestNN(train_data, test_data, ngram_range=nn_count_params['ngram_range'], \n",
    "                         max_df=nn_count_params['max_df'], min_df=nn_count_params['min_df'], vector='count', \n",
    "                         h1_nodes=nn_count_params['h1_nodes'], optimizer=nn_count_params['optimizer'], \n",
    "                         stopwords=stopwords, title='Count - NN-demo')\n",
    "train_test.train_test_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tfidf_params = {\n",
    "    'ngram_range': (1,2),\n",
    "    'max_df': 0.75,\n",
    "    'min_df': 0,\n",
    "    'h1_nodes': 128,\n",
    "    'optimizer': 'Adadelta'\n",
    "}\n",
    "\n",
    "train_test = TrainTestNN(train_data, test_data, ngram_range=nn_tfidf_params['ngram_range'], \n",
    "                         max_df=nn_tfidf_params['max_df'], min_df=nn_tfidf_params['min_df'], vector='tfidf', \n",
    "                         h1_nodes=nn_tfidf_params['h1_nodes'], optimizer=nn_tfidf_params['optimizer'], \n",
    "                         stopwords=stopwords, title='TF-IDF - NN-demo')\n",
    "train_test.train_test_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
