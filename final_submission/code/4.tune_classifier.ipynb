{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Classifier\n",
    "In this step, hyperparameters of the feature extractors and several classifiers will be tuned using 5-fold cross validation. \n",
    "\n",
    "*Note: See scripts in \"4.tune_classifier\" folder for scripting implementation. The tuning was run with a python script on the CHPC clusters as this step is computationally expensive and time consuming.\n",
    "\n",
    "Feature extractors include:\n",
    "<ul>\n",
    "    <li>Count vectorizer</li>\n",
    "    <li>TF-IDF vectorizer</li>\n",
    "</ul>\n",
    "\n",
    "Classifiers include:\n",
    "<ul>\n",
    "    <li>support vector machine</li>\n",
    "    <li>random forest</li>\n",
    "    <li>neural net</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Feature Extractors\n",
    "### Count vectorizer\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html<br/>\n",
    "Hyperparameters to tune:\n",
    "<ul>\n",
    "    <li>ngram_range</li>\n",
    "    <li>max_df</li>\n",
    "    <li>min_df</li>\n",
    "</ul>\n",
    "\n",
    "### TF-IDF vectorizer\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html<br/>\n",
    "Hyperparameters to tune:\n",
    "<ul>\n",
    "    <li>ngram_range</li>\n",
    "    <li>max_df</li>\n",
    "    <li>min_df</li>\n",
    "</ul>\n",
    "\n",
    "Note: Was going to tune max_features as well but forgot to put it into vectorizer call. May look at this later.\n",
    "\n",
    "## Classifiers\n",
    "### Support Vector Machine\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html<br/>\n",
    "Hyperparameters to tune:\n",
    "<ul>\n",
    "    <li>C</li>\n",
    "    <li>kernel</li>\n",
    "    <li>degree</li>\n",
    "    <li>gamma</li>\n",
    "</ul>\n",
    "\n",
    "### Random Forest\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html<br/>\n",
    "Hyperparameters to tune:\n",
    "<ul>\n",
    "    <li>n_estimators</li>\n",
    "    <li>criterion</li>\n",
    "    <li>max_depth</li>\n",
    "</ul>\n",
    "\n",
    "### Neural Net\n",
    "Reference: This one will use keras with tensorflow, rather than sci-kit learn. Need to still extract features with sklearn and return those vectors. Then feed them to keras.<br/>\n",
    "Hyperparameters to tune:\n",
    "<ul>\n",
    "    <li>number of nodes in hidden layer</li>\n",
    "    <li>optimizer</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and stopwords\n",
    "train_data = pd.read_pickle('../data/train_data.pkl')\n",
    "with open('../data/stopwords.pkl', 'rb') as f:\n",
    "    stopwords = pickle.load(f)\n",
    "    \n",
    "# smaller subset for testing/demo\n",
    "train_data = train_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneSVM(object):\n",
    "    def __init__(self, train_data, kernel, cv_num, stopwords, title):\n",
    "        self.data = train_data\n",
    "        self.kernel = kernel\n",
    "        self.stopwords = stopwords\n",
    "        self.title = title\n",
    "        self.k_folds = KFold(n_splits=cv_num, shuffle=True)\n",
    "        self.cv_scores = pd.DataFrame()\n",
    "\n",
    "    def tune_parameters(self, params, vector):\n",
    "        ngram_range = params['ngram_range']\n",
    "        max_df = params['max_df']\n",
    "        min_df = params['min_df']\n",
    "\n",
    "        C = params['C']\n",
    "\n",
    "        for n in ngram_range:\n",
    "            for mx in max_df:\n",
    "                for mn in min_df:\n",
    "                    for c in C:\n",
    "                        self.run_cv(n, mx, mn, c, vector)\n",
    "        return None\n",
    "\n",
    "    def save_scores_csv(self, title):\n",
    "        self.cv_scores.to_csv('../results/tuning/%s_tuning.csv' % title)\n",
    "        return None\n",
    "\n",
    "    def run_cv(self, ngram_range, max_df, min_df, C, vector):\n",
    "        fold = 0\n",
    "        for train_index, val_index in self.k_folds.split(self.data):\n",
    "            fold += 1\n",
    "#             print(fold)\n",
    "            X_train = self.data.iloc[train_index]['text'].values\n",
    "            y_train = self.data.iloc[train_index]['label'].values\n",
    "            X_val = self.data.iloc[val_index]['text'].values\n",
    "            y_val = self.data.iloc[val_index]['label'].values\n",
    "\n",
    "            if vector == 'count':\n",
    "                vectorizer = CountVectorizer(ngram_range=ngram_range,\n",
    "                                             max_df=max_df,\n",
    "                                             min_df=min_df,\n",
    "                                             stop_words=self.stopwords)\n",
    "            else:\n",
    "                vectorizer = TfidfVectorizer(ngram_range=ngram_range,\n",
    "                                             max_df=max_df,\n",
    "                                             min_df=min_df,\n",
    "                                             stop_words=self.stopwords)\n",
    "\n",
    "            try:\n",
    "                X_train_vec = vectorizer.fit_transform(X_train)\n",
    "                X_val_vec = vectorizer.transform(X_val)\n",
    "            except:\n",
    "                return None\n",
    "            else:\n",
    "                clf = SVC(C=C, kernel=self.kernel, probability=True, gamma='scale')\n",
    "                clf.fit(X_train_vec, y_train)\n",
    "\n",
    "                y_train_pred = clf.predict(X_train_vec)\n",
    "                y_train_prob = clf.predict_proba(X_train_vec)\n",
    "                y_train_prob = y_train_prob[:, 1]\n",
    "                train_scores = self.evaluate_cv_results(y_train, y_train_pred, y_train_prob,\n",
    "                                                        ngram_range, max_df, min_df, C)\n",
    "\n",
    "                y_val_pred = clf.predict(X_val_vec)\n",
    "                y_val_prob = clf.predict_proba(X_val_vec)\n",
    "                y_val_prob = y_val_prob[:, 1]\n",
    "                val_scores = self.evaluate_cv_results(y_val, y_val_pred, y_val_prob,\n",
    "                                                      ngram_range, max_df, min_df, C)\n",
    "\n",
    "                eval_df = self.create_scores_dataframe(train_scores, val_scores, fold, vector)\n",
    "                self.cv_scores = pd.concat([self.cv_scores, eval_df])\n",
    "                self.save_scores_csv('temp_%s' % self.title)\n",
    "        return None\n",
    "\n",
    "    def evaluate_cv_results(self, y_true, y_pred, y_prob, ngram_range, max_df, min_df, C):\n",
    "        scores = {'ngram_range': [], 'max_df': [], 'min_df': [], 'C': [],\n",
    "                  'Acc': [], 'recall': [], 'PPV': [], 'AUC': []}\n",
    "\n",
    "        scores['ngram_range'].append(ngram_range)\n",
    "        scores['max_df'].append(max_df)\n",
    "        scores['min_df'].append(min_df)\n",
    "        scores['C'].append(C)\n",
    "        scores['Acc'].append(accuracy_score(y_true, y_pred))\n",
    "        scores['recall'].append(recall_score(y_true, y_pred))\n",
    "        scores['PPV'].append(precision_score(y_true, y_pred))\n",
    "        scores['AUC'].append(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def create_scores_dataframe(self, train_dict, val_dict, fold, vector):\n",
    "        train_df = pd.DataFrame(train_dict)\n",
    "        train_df['dataset'] = 'train'\n",
    "        train_df['fold'] = fold\n",
    "        train_df['vector'] = vector\n",
    "\n",
    "        val_df = pd.DataFrame(val_dict)\n",
    "        val_df['dataset'] = 'val'\n",
    "        val_df['fold'] = fold\n",
    "        val_df['vector'] = vector\n",
    "        eval_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "        return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_params = {\n",
    "#     'ngram_range':[(1,1),(1,2),(2,2)],\n",
    "#     'max_df':np.linspace(0, 1, 5),\n",
    "#     'min_df':np.linspace(0, 1, 5),\n",
    "#     'C':np.linspace(0.01, 5, 5)\n",
    "# }\n",
    "\n",
    "# sample set of parameters for demo\n",
    "svm_params = {\n",
    "    'ngram_range':[(1,1)],\n",
    "    'max_df':[0.5],\n",
    "    'min_df':[0],\n",
    "    'C':[1.0]\n",
    "}\n",
    "\n",
    "tune_psvm = TuneSVM(train_data, 'poly', 3, stopwords, 'psvm_demo')\n",
    "tune_psvm.tune_parameters(svm_params, 'count')\n",
    "tune_psvm.tune_parameters(svm_params, 'tfidf')\n",
    "tune_psvm.save_scores_csv('psvm_demo')\n",
    "\n",
    "tune_lsvm = TuneSVM(train_data, 'linear', 3, stopwords, 'lsvm_demo')\n",
    "tune_lsvm.tune_parameters(svm_params, 'count')\n",
    "tune_lsvm.tune_parameters(svm_params, 'tfidf')\n",
    "tune_lsvm.save_scores_csv('lsvm_demo')\n",
    "\n",
    "tune_rsvm = TuneSVM(train_data, 'rbf', 3, stopwords, 'rsvm_demo')\n",
    "tune_rsvm.tune_parameters(svm_params, 'count')\n",
    "tune_rsvm.tune_parameters(svm_params, 'tfidf')\n",
    "tune_rsvm.save_scores_csv('rsvm_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneRandomForest(object):\n",
    "    def __init__(self, train_data, cv_num, stopwords, title):\n",
    "        self.data = train_data\n",
    "        self.stopwords = stopwords\n",
    "        self.title = title\n",
    "        self.k_folds = KFold(n_splits=cv_num, shuffle=True)\n",
    "        self.cv_scores = pd.DataFrame()\n",
    "        \n",
    "    def tune_parameters(self, params, vector):\n",
    "        ngram_range = params['ngram_range']\n",
    "        max_df = params['max_df']\n",
    "        min_df = params['min_df']\n",
    "        \n",
    "        n_estimators = params['n_estimators']\n",
    "        criterion = params['criterion']\n",
    "        max_depth = params['max_depth']\n",
    "\n",
    "        for n in ngram_range:\n",
    "            for mx in max_df:\n",
    "                for mn in min_df:\n",
    "                    for nest in n_estimators:\n",
    "                        for c in criterion:\n",
    "                            for mxd in max_depth:\n",
    "                                self.run_cv(n, mx, mn, nest, c, mxd, vector)\n",
    "        return None\n",
    "\n",
    "    def save_scores_csv(self, title):\n",
    "        self.cv_scores.to_csv('../results/tuning/%s_tuning.csv' %title)\n",
    "        return None\n",
    "    \n",
    "    def run_cv(self, ngram_range, max_df, min_df, n_estimators, criterion, max_depth, vector):\n",
    "        fold = 0\n",
    "        for train_index, val_index in self.k_folds.split(self.data):\n",
    "            fold += 1\n",
    "#             print(fold)\n",
    "            X_train = self.data.iloc[train_index]['text'].values\n",
    "            y_train = self.data.iloc[train_index]['label'].values\n",
    "            X_val = self.data.iloc[val_index]['text'].values\n",
    "            y_val = self.data.iloc[val_index]['label'].values\n",
    "            \n",
    "            if vector == 'count':\n",
    "                vectorizer = CountVectorizer(ngram_range=ngram_range, \n",
    "                                             max_df=max_df, \n",
    "                                             min_df=min_df, \n",
    "                                             stop_words=self.stopwords)\n",
    "            else:\n",
    "                vectorizer = TfidfVectorizer(ngram_range=ngram_range, \n",
    "                                             max_df=max_df, \n",
    "                                             min_df=min_df, \n",
    "                                             stop_words=self.stopwords)\n",
    "            \n",
    "            try:\n",
    "                X_train_vec = vectorizer.fit_transform(X_train)\n",
    "                X_val_vec = vectorizer.transform(X_val)\n",
    "            except:\n",
    "                return None\n",
    "            else:\n",
    "                clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth)\n",
    "                clf.fit(X_train_vec, y_train)\n",
    "\n",
    "                y_train_pred = clf.predict(X_train_vec)\n",
    "                y_train_prob = clf.predict_proba(X_train_vec)\n",
    "                y_train_prob = y_train_prob[:, 1]\n",
    "                train_scores = self.evaluate_cv_results(y_train, y_train_pred, y_train_prob, \n",
    "                                                        ngram_range, max_df, min_df, \n",
    "                                                        n_estimators, criterion, max_depth)\n",
    "\n",
    "                y_val_pred = clf.predict(X_val_vec)\n",
    "                y_val_prob = clf.predict_proba(X_val_vec)\n",
    "                y_val_prob = y_val_prob[:, 1]\n",
    "                val_scores = self.evaluate_cv_results(y_val, y_val_pred, y_val_prob, \n",
    "                                                      ngram_range, max_df, min_df,  \n",
    "                                                      n_estimators, criterion, max_depth)\n",
    "\n",
    "                eval_df = self.create_scores_dataframe(train_scores, val_scores, fold, vector)\n",
    "                self.cv_scores = pd.concat([self.cv_scores, eval_df])\n",
    "                self.save_scores_csv('temp_%s' %self.title)\n",
    "        return None\n",
    "    \n",
    "    def evaluate_cv_results(self, y_true, y_pred, y_prob, ngram_range, max_df, min_df,  \n",
    "                            n_estimators, criterion, max_depth):\n",
    "        scores = {'ngram_range':[],'max_df':[],'min_df':[],'n_estimators':[],'criterion':[],\n",
    "                  'max_depth':[],'Acc':[],'recall':[],'PPV':[],'AUC':[]}\n",
    "\n",
    "        scores['ngram_range'].append(ngram_range)\n",
    "        scores['max_df'].append(max_df)\n",
    "        scores['min_df'].append(min_df)\n",
    "        scores['n_estimators'].append(n_estimators)\n",
    "        scores['criterion'].append(criterion)\n",
    "        scores['max_depth'].append(max_depth)\n",
    "        scores['Acc'].append(accuracy_score(y_true, y_pred))\n",
    "        scores['recall'].append(recall_score(y_true, y_pred))\n",
    "        scores['PPV'].append(precision_score(y_true, y_pred))\n",
    "        scores['AUC'].append(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def create_scores_dataframe(self, train_dict, val_dict, fold, vector):\n",
    "        train_df = pd.DataFrame(train_dict)\n",
    "        train_df['dataset'] = 'train'\n",
    "        train_df['fold'] = fold\n",
    "        train_df['vector'] = vector\n",
    "\n",
    "        val_df = pd.DataFrame(val_dict)\n",
    "        val_df['dataset'] = 'val'\n",
    "        val_df['fold'] = fold\n",
    "        val_df['vector'] = vector\n",
    "        eval_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "        return eval_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_params = {\n",
    "#     'ngram_range':[(1,1),(1,2),(2,2)],\n",
    "#     'max_df':np.linspace(0, 1, 5),\n",
    "#     'min_df':np.linspace(0, 1, 5),\n",
    "#     'n_estimators':[10, 25, 50, 100, 300],\n",
    "#     'criterion':['gini','entropy'],\n",
    "#     'max_depth':[2, 10, 20, 50, 100]\n",
    "# }\n",
    "\n",
    "# sample set of parameters for demo\n",
    "rf_params = {\n",
    "    'ngram_range':[(1,1)],\n",
    "    'max_df':[0.5],\n",
    "    'min_df':[0],\n",
    "    'max_features':[None],\n",
    "    'n_estimators':[10],\n",
    "    'criterion':['gini'],\n",
    "    'max_depth':[2]\n",
    "}\n",
    "\n",
    "tune_rf = TuneRandomForest(train_data, 3, stopwords, 'rf_demo')\n",
    "tune_rf.tune_parameters(rf_params, 'count')\n",
    "tune_rf.tune_parameters(rf_params, 'tfidf')\n",
    "tune_rf.save_scores_csv('rf_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneNeuralNet(object):\n",
    "    def __init__(self, train_data, cv_num, stopwords, title):\n",
    "        self.data = train_data\n",
    "        self.stopwords = stopwords\n",
    "        self.title = title\n",
    "        self.k_folds = KFold(n_splits=cv_num, shuffle=True)\n",
    "        self.cv_scores = pd.DataFrame()\n",
    "        \n",
    "    def tune_parameters(self, params, vector):\n",
    "        ngram_range = params['ngram_range']\n",
    "        max_df = params['max_df']\n",
    "        min_df = params['min_df']\n",
    "        \n",
    "        h1_nodes = params['h1_nodes']\n",
    "        optimizer = params['optimizer']\n",
    "\n",
    "        for n in ngram_range:\n",
    "            for mx in max_df:\n",
    "                for mn in min_df:\n",
    "                    for h1 in h1_nodes:\n",
    "                        for o in optimizer:\n",
    "                            self.run_cv(n, mx, mn, h1, o, vector)\n",
    "        return None\n",
    "\n",
    "    def save_scores_csv(self, title):\n",
    "        self.cv_scores.to_csv('../results/tuning/%s_tuning.csv' %title)\n",
    "        return None\n",
    "    \n",
    "    def run_cv(self, ngram_range, max_df, min_df, h1_nodes, optimizer, vector):\n",
    "        fold = 0\n",
    "        for train_index, val_index in self.k_folds.split(self.data):\n",
    "            fold += 1\n",
    "#             print(fold)\n",
    "            X_train = self.data.iloc[train_index]['text'].values\n",
    "            y_train = self.data.iloc[train_index]['label'].values\n",
    "            X_val = self.data.iloc[val_index]['text'].values\n",
    "            y_val = self.data.iloc[val_index]['label'].values\n",
    "            \n",
    "            if vector == 'count':\n",
    "                vectorizer = CountVectorizer(ngram_range=ngram_range, \n",
    "                                             max_df=max_df, \n",
    "                                             min_df=min_df, \n",
    "                                             stop_words=self.stopwords)\n",
    "            else:\n",
    "                vectorizer = TfidfVectorizer(ngram_range=ngram_range, \n",
    "                                             max_df=max_df, \n",
    "                                             min_df=min_df, \n",
    "                                             stop_words=self.stopwords)\n",
    "            \n",
    "            try:\n",
    "                X_train_vec = vectorizer.fit_transform(X_train)\n",
    "                X_val_vec = vectorizer.transform(X_val)\n",
    "            except:\n",
    "                return None\n",
    "            else:\n",
    "                n_dim = X_train_vec.shape[1]\n",
    "                early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3)\n",
    "                \n",
    "                model = Sequential()\n",
    "                model.add(Dense(h1_nodes, activation='relu', input_dim=n_dim))\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "                history = model.fit(X_train_vec, y_train, epochs=3000, validation_split=0.2, batch_size=100, \n",
    "                                    callbacks=[early_stopping_monitor], verbose=0)\n",
    "                \n",
    "                \n",
    "                y_train_prob = model.predict(X_train_vec).flatten()\n",
    "                y_train_pred = model.predict_classes(X_train_vec).flatten()\n",
    "                train_scores = self.evaluate_cv_results(y_train, y_train_pred, y_train_prob, \n",
    "                                                        ngram_range, max_df, min_df, \n",
    "                                                        h1_nodes, optimizer)\n",
    "\n",
    "                y_val_prob = model.predict(X_val_vec).flatten()\n",
    "                y_val_pred = model.predict_classes(X_val_vec).flatten()\n",
    "                val_scores = self.evaluate_cv_results(y_val, y_val_pred, y_val_prob, \n",
    "                                                      ngram_range, max_df, min_df, \n",
    "                                                      h1_nodes, optimizer)\n",
    "\n",
    "                eval_df = self.create_scores_dataframe(train_scores, val_scores, fold, vector)\n",
    "                self.cv_scores = pd.concat([self.cv_scores, eval_df])\n",
    "                self.save_scores_csv('temp_%s' %self.title)\n",
    "        return None\n",
    "    \n",
    "    def evaluate_cv_results(self, y_true, y_pred, y_prob, ngram_range, max_df, min_df, \n",
    "                            h1_nodes, optimizer):\n",
    "        scores = {'ngram_range':[],'max_df':[],'min_df':[],'h1_nodes':[],'optimizer':[],\n",
    "                  'Acc':[],'recall':[],'PPV':[],'AUC':[]}\n",
    "\n",
    "        scores['ngram_range'].append(ngram_range)\n",
    "        scores['max_df'].append(max_df)\n",
    "        scores['min_df'].append(min_df)\n",
    "        scores['h1_nodes'].append(h1_nodes)\n",
    "        scores['optimizer'].append(optimizer)\n",
    "        scores['Acc'].append(accuracy_score(y_true, y_pred))\n",
    "        scores['recall'].append(recall_score(y_true, y_pred))\n",
    "        scores['PPV'].append(precision_score(y_true, y_pred))\n",
    "        scores['AUC'].append(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def create_scores_dataframe(self, train_dict, val_dict, fold, vector):\n",
    "        train_df = pd.DataFrame(train_dict)\n",
    "        train_df['dataset'] = 'train'\n",
    "        train_df['fold'] = fold\n",
    "        train_df['vector'] = vector\n",
    "\n",
    "        val_df = pd.DataFrame(val_dict)\n",
    "        val_df['dataset'] = 'val'\n",
    "        val_df['fold'] = fold\n",
    "        val_df['vector'] = vector\n",
    "        eval_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "        return eval_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nn_params = {\n",
    "#     'ngram_range':[(1,1),(1,2),(2,2)],\n",
    "#     'max_df':np.linspace(0, 1, 5),\n",
    "#     'min_df':np.linspace(0, 1, 5),\n",
    "#     'h1_nodes':[128, 512, 1024, 2048, 3200],\n",
    "#     'optimizer':['Adam','RMSprop','Adadelta']\n",
    "# }\n",
    "\n",
    "# sample set of parameters for demo\n",
    "nn_params = {\n",
    "    'ngram_range':[(1,1)],\n",
    "    'max_df':[0.5],\n",
    "    'min_df':[0],\n",
    "    'h1_nodes':[128],\n",
    "    'optimizer':['Adam']\n",
    "}\n",
    "\n",
    "tune_nn = TuneNeuralNet(train_data, 3, stopwords, 'nn_demo')\n",
    "tune_nn.tune_parameters(nn_params, 'count')\n",
    "tune_nn.tune_parameters(nn_params, 'tfidf')\n",
    "tune_nn.save_scores_csv('nn_demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Count Vectorizer, Best parameters for accuracy:\n",
      "ngram_range=(1, 1)\n",
      "max_df=1.000000\n",
      "min_df=0.000000\n",
      "n_estimators=300\n",
      "criterion=gini\n",
      "max_depth=100\n",
      "Scores: Acc=0.8601, AUC=0.9389, Recall=0.9313, PPV=0.8209\n",
      "\n",
      "Random Forest - Count Vectorizer, Best parameters for AUC:\n",
      "ngram_range=(1, 1)\n",
      "max_df=0.750000\n",
      "min_df=0.000000\n",
      "n_estimators=300\n",
      "criterion=entropy\n",
      "max_depth=100\n",
      "Scores: Acc=0.8597, AUC=0.9407, Recall=0.9353, PPV=0.8182\n",
      "\n",
      "\n",
      "Random Forest - TF-IDF Vectorizer, Best parameters for accuracy:\n",
      "ngram_range=(1, 1)\n",
      "max_df=0.500000\n",
      "min_df=0.000000\n",
      "n_estimators=300\n",
      "criterion=entropy\n",
      "max_depth=100\n",
      "Scores: Acc=0.8601, AUC=0.9403, Recall=0.9325, PPV=0.8203\n",
      "\n",
      "Random Forest - TF-IDF Vectorizer, Best parameters for AUC:\n",
      "ngram_range=(1, 1)\n",
      "max_df=1.000000\n",
      "min_df=0.000000\n",
      "n_estimators=300\n",
      "criterion=entropy\n",
      "max_depth=100\n",
      "Scores: Acc=0.8596, AUC=0.9405, Recall=0.9317, PPV=0.8201\n"
     ]
    }
   ],
   "source": [
    "rf = pd.read_csv('../results/tuning/rf_tuning.csv')\n",
    "rf.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "rf_count = rf.loc[(rf['vector'] == 'count') & (rf['dataset'] == 'val')].copy()\n",
    "rf_count.drop(columns='fold', inplace=True)\n",
    "rf_count = rf_count.groupby(['ngram_range','max_df','min_df','n_estimators','criterion','max_depth']).mean()\n",
    "rf_count_acc = rf_count.sort_values('Acc', ascending=False).head(1).reset_index()\n",
    "rf_count_auc = rf_count.sort_values('AUC', ascending=False).head(1).reset_index()\n",
    "print('Random Forest - Count Vectorizer, Best parameters for accuracy:')\n",
    "print('ngram_range=%s' %str(rf_count_acc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %rf_count_acc['max_df'].values[0])\n",
    "print('min_df=%f' %rf_count_acc['min_df'].values[0])\n",
    "print('n_estimators=%d' %rf_count_acc['n_estimators'].values[0])\n",
    "print('criterion=%s' %rf_count_acc['criterion'].values[0])\n",
    "print('max_depth=%d' %rf_count_acc['max_depth'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(rf_count_acc['Acc'].values[0], \n",
    "                                                                rf_count_acc['AUC'].values[0], \n",
    "                                                                rf_count_acc['recall'].values[0], \n",
    "                                                                rf_count_acc['PPV'].values[0]))\n",
    "print('')\n",
    "\n",
    "print('Random Forest - Count Vectorizer, Best parameters for AUC:')\n",
    "print('ngram_range=%s' %str(rf_count_auc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %rf_count_auc['max_df'].values[0])\n",
    "print('min_df=%f' %rf_count_auc['min_df'].values[0])\n",
    "print('n_estimators=%d' %rf_count_auc['n_estimators'].values[0])\n",
    "print('criterion=%s' %rf_count_auc['criterion'].values[0])\n",
    "print('max_depth=%d' %rf_count_auc['max_depth'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(rf_count_auc['Acc'].values[0], \n",
    "                                                                rf_count_auc['AUC'].values[0], \n",
    "                                                                rf_count_auc['recall'].values[0], \n",
    "                                                                rf_count_auc['PPV'].values[0]))\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "rf_tfidf = rf.loc[(rf['vector'] == 'tfidf') & (rf['dataset'] == 'val')].copy()\n",
    "rf_tfidf.drop(columns='fold', inplace=True)\n",
    "rf_tfidf = rf_tfidf.groupby(['ngram_range','max_df','min_df','n_estimators','criterion','max_depth']).mean()\n",
    "rf_tfidf_acc = rf_tfidf.sort_values('Acc', ascending=False).head(1).reset_index()\n",
    "rf_tfidf_auc = rf_tfidf.sort_values('AUC', ascending=False).head(1).reset_index()\n",
    "print('Random Forest - TF-IDF Vectorizer, Best parameters for accuracy:')\n",
    "print('ngram_range=%s' %str(rf_tfidf_acc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %rf_tfidf_acc['max_df'].values[0])\n",
    "print('min_df=%f' %rf_tfidf_acc['min_df'].values[0])\n",
    "print('n_estimators=%d' %rf_tfidf_acc['n_estimators'].values[0])\n",
    "print('criterion=%s' %rf_tfidf_acc['criterion'].values[0])\n",
    "print('max_depth=%d' %rf_tfidf_acc['max_depth'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(rf_tfidf_acc['Acc'].values[0], \n",
    "                                                                rf_tfidf_acc['AUC'].values[0], \n",
    "                                                                rf_tfidf_acc['recall'].values[0], \n",
    "                                                                rf_tfidf_acc['PPV'].values[0]))\n",
    "print('')\n",
    "\n",
    "print('Random Forest - TF-IDF Vectorizer, Best parameters for AUC:')\n",
    "print('ngram_range=%s' %str(rf_tfidf_auc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %rf_tfidf_auc['max_df'].values[0])\n",
    "print('min_df=%f' %rf_tfidf_auc['min_df'].values[0])\n",
    "print('n_estimators=%d' %rf_tfidf_auc['n_estimators'].values[0])\n",
    "print('criterion=%s' %rf_tfidf_auc['criterion'].values[0])\n",
    "print('max_depth=%d' %rf_tfidf_auc['max_depth'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(rf_tfidf_auc['Acc'].values[0], \n",
    "                                                                rf_tfidf_auc['AUC'].values[0], \n",
    "                                                                rf_tfidf_auc['recall'].values[0], \n",
    "                                                                rf_tfidf_auc['PPV'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:\n",
    "For Count Vectorizer:\n",
    "<ul>\n",
    "    <li>ngram_range: (1, 1)</li>\n",
    "    <li>max_df: 1.0</li>\n",
    "    <li>min_df: 0</li>\n",
    "    <li>n_estimators: 300</li>\n",
    "    <li>criterion: gini</li>\n",
    "    <li>max_depth: 100</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<br/>\n",
    "For TF-IDF Vectorizer:\n",
    "<ul>\n",
    "    <li>ngram_range: (1, 1)</li>\n",
    "    <li>max_df: 0.5</li>\n",
    "    <li>min_df: 0</li>\n",
    "    <li>n_estimators: 300</li>\n",
    "    <li>criterion: entropy</li>\n",
    "    <li>max_depth: 100</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Count Vectorizer, Best parameters for accuracy:\n",
      "ngram_range=(1, 2)\n",
      "max_df=0.500000\n",
      "min_df=0.000000\n",
      "C=0.010000\n",
      "kernel=linear\n",
      "Scores: Acc=0.8476, AUC=0.9392, Recall=0.9410, PPV=0.7988\n",
      "\n",
      "SVM - Count Vectorizer, Best parameters for AUC:\n",
      "ngram_range=(1, 2)\n",
      "max_df=0.500000\n",
      "min_df=0.000000\n",
      "C=0.010000\n",
      "kernel=linear\n",
      "Scores: Acc=0.8476, AUC=0.9392, Recall=0.9410, PPV=0.7988\n",
      "\n",
      "\n",
      "Random Forest - TF-IDF Vectorizer, Best parameters for accuracy:\n",
      "ngram_range=(1, 2)\n",
      "max_df=0.250000\n",
      "min_df=0.000000\n",
      "C=1.257500\n",
      "kernel=linear\n",
      "Scores: Acc=0.8720, AUC=0.9454, Recall=0.8962, PPV=0.8609\n",
      "\n",
      "Random Forest - TF-IDF Vectorizer, Best parameters for AUC:\n",
      "ngram_range=(1, 2)\n",
      "max_df=0.250000\n",
      "min_df=0.000000\n",
      "C=1.257500\n",
      "kernel=linear\n",
      "Scores: Acc=0.8720, AUC=0.9454, Recall=0.8962, PPV=0.8609\n"
     ]
    }
   ],
   "source": [
    "svm = pd.read_csv('../results/tuning/svm_tuning.csv')\n",
    "svm.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "svm_count = svm.loc[(svm['vector'] == 'count') & (svm['dataset'] == 'val')].copy()\n",
    "svm_count.drop(columns='fold', inplace=True)\n",
    "svm_count = svm_count.groupby(['ngram_range','max_df','min_df','C','kernel']).mean()\n",
    "svm_count_acc = svm_count.sort_values('Acc', ascending=False).head(1).reset_index()\n",
    "svm_count_auc = svm_count.sort_values('AUC', ascending=False).head(1).reset_index()\n",
    "print('SVM - Count Vectorizer, Best parameters for accuracy:')\n",
    "print('ngram_range=%s' %str(svm_count_acc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %svm_count_acc['max_df'].values[0])\n",
    "print('min_df=%f' %svm_count_acc['min_df'].values[0])\n",
    "print('C=%f' %svm_count_acc['C'].values[0])\n",
    "print('kernel=%s' %svm_count_acc['kernel'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(svm_count_acc['Acc'].values[0], \n",
    "                                                                svm_count_acc['AUC'].values[0], \n",
    "                                                                svm_count_acc['recall'].values[0], \n",
    "                                                                svm_count_acc['PPV'].values[0]))\n",
    "print('')\n",
    "\n",
    "print('SVM - Count Vectorizer, Best parameters for AUC:')\n",
    "print('ngram_range=%s' %str(svm_count_auc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %svm_count_auc['max_df'].values[0])\n",
    "print('min_df=%f' %svm_count_auc['min_df'].values[0])\n",
    "print('C=%f' %svm_count_auc['C'].values[0])\n",
    "print('kernel=%s' %svm_count_auc['kernel'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(svm_count_auc['Acc'].values[0], \n",
    "                                                                svm_count_auc['AUC'].values[0], \n",
    "                                                                svm_count_auc['recall'].values[0], \n",
    "                                                                svm_count_auc['PPV'].values[0]))\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "svm_tfidf = svm.loc[(svm['vector'] == 'tfidf') & (svm['dataset'] == 'val')].copy()\n",
    "svm_tfidf.drop(columns='fold', inplace=True)\n",
    "svm_tfidf = svm_tfidf.groupby(['ngram_range','max_df','min_df','C','kernel']).mean()\n",
    "svm_tfidf_acc = svm_tfidf.sort_values('Acc', ascending=False).head(1).reset_index()\n",
    "svm_tfidf_auc = svm_tfidf.sort_values('AUC', ascending=False).head(1).reset_index()\n",
    "print('Random Forest - TF-IDF Vectorizer, Best parameters for accuracy:')\n",
    "print('ngram_range=%s' %str(svm_tfidf_acc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %svm_tfidf_acc['max_df'].values[0])\n",
    "print('min_df=%f' %svm_tfidf_acc['min_df'].values[0])\n",
    "print('C=%f' %svm_tfidf_acc['C'].values[0])\n",
    "print('kernel=%s' %svm_tfidf_acc['kernel'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(svm_tfidf_acc['Acc'].values[0], \n",
    "                                                                svm_tfidf_acc['AUC'].values[0], \n",
    "                                                                svm_tfidf_acc['recall'].values[0], \n",
    "                                                                svm_tfidf_acc['PPV'].values[0]))\n",
    "print('')\n",
    "\n",
    "print('Random Forest - TF-IDF Vectorizer, Best parameters for AUC:')\n",
    "print('ngram_range=%s' %str(svm_tfidf_auc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %svm_tfidf_auc['max_df'].values[0])\n",
    "print('min_df=%f' %svm_tfidf_auc['min_df'].values[0])\n",
    "print('C=%f' %svm_tfidf_auc['C'].values[0])\n",
    "print('kernel=%s' %svm_tfidf_auc['kernel'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(svm_tfidf_auc['Acc'].values[0], \n",
    "                                                                svm_tfidf_auc['AUC'].values[0], \n",
    "                                                                svm_tfidf_auc['recall'].values[0], \n",
    "                                                                svm_tfidf_auc['PPV'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM:\n",
    "For Count Vectorizer:\n",
    "<ul>\n",
    "    <li>ngram_range: (1, 2)</li>\n",
    "    <li>max_df: 0.5</li>\n",
    "    <li>min_df: 0</li>\n",
    "    <li>kernel: linear</li>\n",
    "    <li>C: 0.01</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<br/>\n",
    "For TF-IDF Vectorizer:\n",
    "<ul>\n",
    "    <li>ngram_range: (1, 2)</li>\n",
    "    <li>max_df: 0.25</li>\n",
    "    <li>min_df: 0</li>\n",
    "    <li>kernel: linear</li>\n",
    "    <li>C: 1.2575</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN - Count Vectorizer, Best parameters for accuracy:\n",
      "ngram_range=(1, 2)\n",
      "max_df=1.000000\n",
      "min_df=0.000000\n",
      "h1_nodes=128.000000\n",
      "optimizer=Adadelta\n",
      "Scores: Acc=0.8678, AUC=0.9440, Recall=0.8920, PPV=0.8570\n",
      "\n",
      "NN - Count Vectorizer, Best parameters for AUC:\n",
      "ngram_range=(1, 1)\n",
      "max_df=0.500000\n",
      "min_df=0.000000\n",
      "h1_nodes=3200.000000\n",
      "optimizer=Adadelta\n",
      "Scores: Acc=0.8651, AUC=0.9448, Recall=0.8965, PPV=0.8502\n",
      "\n",
      "\n",
      "NN - TF-IDF Vectorizer, Best parameters for accuracy:\n",
      "ngram_range=(1, 2)\n",
      "max_df=0.750000\n",
      "min_df=0.000000\n",
      "h1_nodes=128.000000\n",
      "optimizer=Adadelta\n",
      "Scores: Acc=0.8706, AUC=0.9475, Recall=0.8889, PPV=0.8643\n",
      "\n",
      "NN - TF-IDF Vectorizer, Best parameters for AUC:\n",
      "ngram_range=(1, 2)\n",
      "max_df=1.000000\n",
      "min_df=0.000000\n",
      "h1_nodes=128.000000\n",
      "optimizer=Adadelta\n",
      "Scores: Acc=0.8671, AUC=0.9481, Recall=0.8602, PPV=0.8793\n"
     ]
    }
   ],
   "source": [
    "nn = pd.read_csv('../results/tuning/nn_tuning.csv')\n",
    "nn.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "nn_count = nn.loc[(nn['vector'] == 'count') & (nn['dataset'] == 'val')].copy()\n",
    "nn_count.drop(columns='fold', inplace=True)\n",
    "nn_count = nn_count.groupby(['ngram_range','max_df','min_df','h1_nodes','optimizer']).mean()\n",
    "nn_count_acc = nn_count.sort_values('Acc', ascending=False).head(1).reset_index()\n",
    "nn_count_auc = nn_count.sort_values('AUC', ascending=False).head(1).reset_index()\n",
    "print('NN - Count Vectorizer, Best parameters for accuracy:')\n",
    "print('ngram_range=%s' %str(nn_count_acc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %nn_count_acc['max_df'].values[0])\n",
    "print('min_df=%f' %nn_count_acc['min_df'].values[0])\n",
    "print('h1_nodes=%f' %nn_count_acc['h1_nodes'].values[0])\n",
    "print('optimizer=%s' %nn_count_acc['optimizer'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(nn_count_acc['Acc'].values[0], \n",
    "                                                                nn_count_acc['AUC'].values[0], \n",
    "                                                                nn_count_acc['recall'].values[0], \n",
    "                                                                nn_count_acc['PPV'].values[0]))\n",
    "print('')\n",
    "\n",
    "print('NN - Count Vectorizer, Best parameters for AUC:')\n",
    "print('ngram_range=%s' %str(nn_count_auc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %nn_count_auc['max_df'].values[0])\n",
    "print('min_df=%f' %nn_count_auc['min_df'].values[0])\n",
    "print('h1_nodes=%f' %nn_count_auc['h1_nodes'].values[0])\n",
    "print('optimizer=%s' %nn_count_auc['optimizer'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(nn_count_auc['Acc'].values[0], \n",
    "                                                                nn_count_auc['AUC'].values[0], \n",
    "                                                                nn_count_auc['recall'].values[0], \n",
    "                                                                nn_count_auc['PPV'].values[0]))\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "nn_tfidf = nn.loc[(nn['vector'] == 'tfidf') & (nn['dataset'] == 'val')].copy()\n",
    "nn_tfidf.drop(columns='fold', inplace=True)\n",
    "nn_tfidf = nn_tfidf.groupby(['ngram_range','max_df','min_df','h1_nodes','optimizer']).mean()\n",
    "nn_tfidf_acc = nn_tfidf.sort_values('Acc', ascending=False).head(1).reset_index()\n",
    "nn_tfidf_auc = nn_tfidf.sort_values('AUC', ascending=False).head(1).reset_index()\n",
    "print('NN - TF-IDF Vectorizer, Best parameters for accuracy:')\n",
    "print('ngram_range=%s' %str(nn_tfidf_acc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %nn_tfidf_acc['max_df'].values[0])\n",
    "print('min_df=%f' %nn_tfidf_acc['min_df'].values[0])\n",
    "print('h1_nodes=%f' %nn_tfidf_acc['h1_nodes'].values[0])\n",
    "print('optimizer=%s' %nn_tfidf_acc['optimizer'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(nn_tfidf_acc['Acc'].values[0], \n",
    "                                                                nn_tfidf_acc['AUC'].values[0], \n",
    "                                                                nn_tfidf_acc['recall'].values[0], \n",
    "                                                                nn_tfidf_acc['PPV'].values[0]))\n",
    "print('')\n",
    "\n",
    "print('NN - TF-IDF Vectorizer, Best parameters for AUC:')\n",
    "print('ngram_range=%s' %str(nn_tfidf_auc['ngram_range'].values[0]))\n",
    "print('max_df=%f' %nn_tfidf_auc['max_df'].values[0])\n",
    "print('min_df=%f' %nn_tfidf_auc['min_df'].values[0])\n",
    "print('h1_nodes=%f' %nn_tfidf_auc['h1_nodes'].values[0])\n",
    "print('optimizer=%s' %nn_tfidf_auc['optimizer'].values[0])\n",
    "print('Scores: Acc=%.04f, AUC=%.04f, Recall=%.04f, PPV=%.04f' %(nn_tfidf_auc['Acc'].values[0], \n",
    "                                                                nn_tfidf_auc['AUC'].values[0], \n",
    "                                                                nn_tfidf_auc['recall'].values[0], \n",
    "                                                                nn_tfidf_auc['PPV'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN:\n",
    "For Count Vectorizer:\n",
    "<ul>\n",
    "    <li>ngram_range: (1, 2)</li>\n",
    "    <li>max_df: 1.0</li>\n",
    "    <li>min_df: 0</li>\n",
    "    <li>h1_nodes: 128</li>\n",
    "    <li>optimizer: Adadelta</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<br/>\n",
    "For TF-IDF Vectorizer:\n",
    "<ul>\n",
    "    <li>ngram_range: (1, 2)</li>\n",
    "    <li>max_df: 0.75</li>\n",
    "    <li>min_df: 0</li>\n",
    "    <li>h1_nodes: 128</li>\n",
    "    <li>optimizer: Adadelta</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
